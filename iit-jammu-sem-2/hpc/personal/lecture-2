1: Overview
2: Limitation on Single Processor Performance
3: Concept of Multiprocessing
4: Various Courses on Multiprocessing
5: Flynn's Taxonomy
6: Measuring Performance
7: Amdahl's Law

: Parallel Solution : Ideally Two Ways for High Performance
- Work/Functional/Task Partitioning:
  Different Processors executing different tasks
- Domain/Data Decompostion:
  Different Processors solving same global task on smaller subsets

: What is a computer? :
- Von Neumann Architecture
  CPU : Process instrns.
    - Register
    - ALU
    - Control Unit
  Memory / RAM : stores Data & Instructions
    - memory is "flat" consecutive locations
    - access on any location has always the same cost
  One CPU , One Memory & One Input/Output units:

: Assumptions in Algorithm Design : 
- 1 instruction/unit time
- Access to memory has same cost as any other Arithmetic/Logic opern.

NOTE : Processor Performance scaling has reached its Limits

: Concept of Multiple Processors : 

Multiprocessing : multiple processors working together
                  - same chip or across different chips

on same chip : chip multi-processor(CMP) or 'core'
on same board(Motherboard): 
  - may share same memory
  - may exchange data via message passing via (preferably, synchronous) 
    communication systems.


: Computer Cluster - Page 1 :
What is a computer cluster?
- Set of computers working together can be viewed as a single system.
- Each node may perform the same task, controlled and scheduled by software.

: Components & Working principles :
- Each node(computer as a server) running its own instance of an operating system.
- nodes use the same hardware and the same OS.
- The components of a cluster are usually connected to each other through fast
local area networks.
- Possible to have different hardware and OSs.

Usages - Advantages
* Usually deployed to improve performance and availability.
* Much more cost-effective than single computers of comparable speed or availability.

Some Important Terms - Single & Multi-User Systems

Single-User system 
------------------
A system that allows only one user to perform task.
Single User system can be Single-Tasking or Multi-Tasking.

* Single User/Single Tasking : 

* Single User/Multi Tasking : 

* Multi User :

: Symmetric vs Asymmetric MULTIPROCESSING... : 

Symmetric processsing : All machines are considered equal.
Asymmetric processing : All machines are not considered equal.

Symmetric Multiprocessors : All processors are same.
                            All consituent processing elements are same.

Hybrid computing : System that has more than one kind of processors or cores.

Strong vs Loosely Coupled Multiprocessing : 

Loosely coupled multiprocessing : running multiple unrelated programs in parallel on a multiprocessor.
Strongly coupled multiprocessing : running a set of programs in parallel that share
                                   their data, code, file and network connections.

Shared Memory : All the programs share the virtual address space.
Message Passing : Programs communicate between each other by sending and 
                  receiving messages. Do not share memory addresses.

Parallel Processing : same clock & control.
- Parallel Architecture
- Parallel Algorithms 

Distributed Systems and Processing : 
- Not colocated
- No common clock & control
- All machines are somehow independent, but all are participating for a common goal

Distributed System Architecture : ??
- How various machines are connected & OS on top of it.
- How are they communicating? Invoking methods from one machine to another and so on.

Distributed Computing :
- Algorithms required to build and maintain distributed architecture and environment.
- Maintaining concurrency of components, overcoming the lack of global clock.
- All communication related algorithms
- Providing mutual exclusions, leader election, coming up with concensus decision.
- Algorithms for fault-tolerant execution.

Distributed Algorithms : 
  Precondition - 
    - Distributed system already in place. 
    - We are trying to write (Applications) algorithms which can use this distributed 
      system to solve problems. 
    - We assume underlying distributed system provides fault-free communication, mutual exclusions. 

  Contents -
    - Algorithms may depend upon timing behaviour of various machines.
      asynchronous, synchronous, semi-synchronous
    - Graph algorithms for Large graphs
    - Sorting algorithms for data distributed over multiple systems

Fault Tolerant Computing :
Fault tolerance refers to ability of a system(computer, network, cloud, cluster, etc)
to continue operating without interruption when one or more of its components fail. 
 - multiple processors are NOT used for high-performance, but used to achieve 
   correct result regardless of faults in the system.
 - objective - improve reliability using multiple systems.

              :: Flynn's Taxonomy :

Flynn's Classification :
* Instruction Stream : Set of instructions to be executed.
* Data Stream : Set of Data values.

Conceptually 4 types of multiprocessors :
_______________
| SISD | SIMD |
| MISD | MIMD |
---------------

SISD : 
  - standard uniprocessor
  - Just Von Neumann Architecture
  - No multiprocessing opportunity

SIMD : 
  - One instruction operates on multiple pieces of data.
  - Vector processors have one instruction that operates on many pieces of data in parallel.
  - Example, one instruction(say, sqr) can compute the square of 4 values in parallel.

MISD :
  - Not used for high performance
  - Used for safety-critical/ fault tolerant/ high-precision systems.
  - Usage :
    - An aircraft with an AMD, an ARM and an INTEL X86 processor
    - final outcome is decided on the basis of majority vote
  
MIMD : Instruction Pool
  
  D          |         |
  A --> PU <-|->  PU <-|
  T          |         |
  A --> PU <-|->  PU <-|
    
  P
  O
  O
  L

Variation on MIMD -
1. SPMD / Single Program Multiple Data
  - Typically multiple processes or threads executing
    the same program with different inputs.
  Ex - pthread, OpenMP, MPI program

2. MPMD / Multiple Programs Multiple Data
  - Different programs run on different machines 
  - Usually, a master program and multiple instances of 
    slave (also called as worker) programs
  - Master delegates work to slaves
  - Master monitors & control slaves, assign tasks, 
    aborting slaves, etc.
  - Possibility of more than one master program, 
    and also other kind organization

Performance - 
Depends on the response time, throughput and execution time of a computer system.

Response Time/ Wall-time : 
- time from start to completion of a task.
- also includes :
  - OS overhead.
  - Waiting for I/O and other processes.
  - Accessing disk and memory.
  - Time spent executing on the CPU or execution time.
  - Throughput is the total amount of work done in given time.

: Execution time and Performance :
Execution time is the total time a CPU spends computing
on a given task 
* It excludes time for I/O or running other programs.


* Execution time : Total time a CPU spends computing on a given task.
  - It excludes time for I/O or running other programs.
  - This is also referred to as simply CPU time.

Performance = 1/ Execution Time


* With only one processor (computing unit)
  - Response time >= Execution time
  - If (Response time == Execution time ){
    no OS overhead, 
    no I/O waiting,
    no other process has put any overhead in the system.
  }


* With Multiple Processors( Computing units)
- Objective : Response time << Execution time
- Execution is supposed to be distributed in some way to 
  multiple processing units - such that the task finishes in less time.
- If (Response time == Execution time 
means some or all of these {
  No practical benefit of using multiple processors,
  various overheads(OS/IO) delaying task completion,
  Other tasks running on the system may have put overhead
}


How much benefit can we get using multiple processors??
- Need to do Performance Comparison. 
- Amdahl's Law

:: Quantification & Definitions::
* Enhancement : It is any change/ modification in the design and realization of component. % of enhancement
fE

* Factor of Improvement : It is a ratio that implies the scale of improvement - fI

* Speed-up (S) : It is a measure that tells us how much
  faster a task will run using the enhanced version of code/machine as opposed to the original machine.
  S = Runtime with enhancement/ Runtime without enhancement
  


Processor A is faster than processor B 

:: Amdahl's Law ::
- a mathematical expression used to find the maximum expected improvement to an overall system when only part of the system is improved/enhanced/running in parallel.
- It is often used in parallel computing to predict the theoretical maximum speed-up using multiple processors.
- For P parallel processors, we can expect a speedup of P(ideal case)
- In practice, we get speed-up <= P because:
  - there are dependencies among instructions/ lines of code
  - dependent instructions must follow/run in order
  - enhanced feature may not be used all the time.
  - maximum number of concurrent instructions (run in parallel) is <= P
  - more that one memory locations are read simultaneously (in
    one clock cycle) by by 1 processor, or multiple processors

Exclusivve Read & Concurrent Read

----
Exclusive Read (ER): no two processors are allowed to read the
same shared memory cell / location / address simultaneously.
Concurrent Read (CR): Simultaneous read by multiple processors
(including all processors) to the same shared memory location are
allowed.
----

No consistency issue in ER or CR
There is no consistency issue with the read (either ER or CR).

----

No consistency issue in ER or CR

Can we write two memory locations simultaneously?
â–¶ Perhaps YES - if the hardware (Memory architecture / controller)
permits.
Can we write the same memory location by two or more processors
simultaneously?
â–¶ NO - What will be the result? Consistency issue.
â–¶ What if all processors want to write the same value - then No Issue.


Let's discuss : Exclusive Read & Concurrent Write
                ER-CW

## Concurrent Write [CW] Variations -

Various methods are used to resolve conflict among the contending
processors trying to write same memory location.

1. Priority based resolution, or, Priority Concurrent Write.
â–¶ Processors are assigned with priorities
2. Arbitrary/Random resolution, or, Arbitrary Concurrent Write
â–¶ one randomly chosen write wins
3. Common/Same value writing by multiple processors (Common
â–¶ Processors are trying to write same value to a memory location.

NOTE : Real memory (computer) system can imitate partially to Common
CW model - at the best.



.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

:: Find an Item in an array ::

Objective - 

We will solve this problem for various PRAM models.
Item may appear one or multiple times
Items may be present in unsorted or sorted in the Array

return - true/false if present or absent
       - first/last/any index where x is present


Find x in an unsorted A[]

-- Sequential solution is trivial
  - take an index = -1, keep updating until it reaches the result
  - return false, if index == -1 else return true

int low = 0, high = n-1

while (low != high):
  m = low + (high - low)//2
  if x == arr[m]:
    return m
  else if x > arr[m]:
    low = m + 1
  else:
    high = m - 1
  

The cost of Sequential algorithm and parallel algorithm in an unsorted array is 
the same. -- cost optimality???


      ## Find Item : Unsorted Array ##

## Find x in Unsorted Array A[] without Duplicate - P1

So, -- All processors are able to write true in one place when if x is present in 
       array because x can be present at only one place. 